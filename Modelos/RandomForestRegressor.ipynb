{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m\n\u001b[0;32m     44\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     45\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     46\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m     47\u001b[0m ])\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Treinar o modelo\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m model_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Fazer previsões\u001b[39;00m\n\u001b[0;32m     53\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 2126231c41efdf0f0a1ccd451ea116b8825667fd
   "source": [
    "# primeira tentativa com random forest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"../Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18521.300670029792\n",
      "MAE: 105.63585213014834\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18136.803027188955\n",
      "MAE: 104.82506590254518\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18007.406524008937\n",
      "MAE: 104.4635009544379\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> 2126231c41efdf0f0a1ccd451ea116b8825667fd
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18.415524473382906\n",
      "MAE: 3.155886588519814\n"
     ]
    }
   ],
   "source": [
    "# o melhor até agora, pois usou o freight_value, mas ta prevendo a data e nao o delivery time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Adicionar a variável de frete (freight_value)\n",
    "df['freight_value'] = df['freight_value']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> 2126231c41efdf0f0a1ccd451ea116b8825667fd
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de previsão extremos: []\n",
      "MSE: 18.4067136434971\n",
      "MAE: 3.1528348047677763\n",
      "CSV de previsões salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# melhor até agora, retorna direto o delivery time\n",
    "# retorna um csv resultado_previsoes que compara o nosso modelo com o antigo fornecido na AWS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Adicionar a variável de frete (freight_value)\n",
    "df['freight_value'] = df['freight_value']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Preencher valores ausentes com a mediana das colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo com regressor RandomForestRegressor\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "model_pipeline.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp','delivery_time_model']), y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = model_pipeline.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Diagnóstico para verificar as previsões\n",
    "print(\"Valores de previsão extremos:\", y_pred[(y_pred < 0) | (y_pred > 365)]) \n",
    "\n",
    "# Limitar valores de previsão a um intervalo razoável, por exemplo, 0 a 365 dias (1 ano)\n",
    "y_pred = np.clip(y_pred, 0, 365)\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('Data/resultado_previsoes.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões salvo com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>Delivery Time Real</th>\n",
       "      <th>Delivery Time Previsto (Antigo)</th>\n",
       "      <th>Delivery Time Previsto (Novo)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18527</th>\n",
       "      <td>4912377c02eb84d181d53e98e010124e</td>\n",
       "      <td>8.81</td>\n",
       "      <td>18.20</td>\n",
       "      <td>9.9720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49324</th>\n",
       "      <td>c25acd72d5c2eb2ec89c3c8d7cd7d1f2</td>\n",
       "      <td>4.26</td>\n",
       "      <td>11.50</td>\n",
       "      <td>7.6606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43893</th>\n",
       "      <td>ad4c4ee12613fc3dc5d506e4e6455498</td>\n",
       "      <td>16.61</td>\n",
       "      <td>41.01</td>\n",
       "      <td>16.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22937</th>\n",
       "      <td>5a5a4a1319679a5e560257abd2fa1273</td>\n",
       "      <td>12.83</td>\n",
       "      <td>25.20</td>\n",
       "      <td>12.8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>48251a875839d15f52e7dcb61e93628a</td>\n",
       "      <td>10.34</td>\n",
       "      <td>32.35</td>\n",
       "      <td>10.8214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>9e5fd9b33580d845c54855ee2e3965b0</td>\n",
       "      <td>15.15</td>\n",
       "      <td>28.38</td>\n",
       "      <td>10.6027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63102</th>\n",
       "      <td>f8b730c3feb3136010e89c9acd18cdad</td>\n",
       "      <td>5.30</td>\n",
       "      <td>17.32</td>\n",
       "      <td>5.0609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12678</th>\n",
       "      <td>3273805a17a78c15811a6feb489662fd</td>\n",
       "      <td>9.06</td>\n",
       "      <td>27.33</td>\n",
       "      <td>9.4697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26392</th>\n",
       "      <td>68233f255c4f267082b2fc19f6449977</td>\n",
       "      <td>26.54</td>\n",
       "      <td>21.96</td>\n",
       "      <td>11.8614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28420</th>\n",
       "      <td>705c73692cee1d4b25042f00e09d7688</td>\n",
       "      <td>8.37</td>\n",
       "      <td>19.64</td>\n",
       "      <td>7.7572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                OrderID  Delivery Time Real  \\\n",
       "18527  4912377c02eb84d181d53e98e010124e                8.81   \n",
       "49324  c25acd72d5c2eb2ec89c3c8d7cd7d1f2                4.26   \n",
       "43893  ad4c4ee12613fc3dc5d506e4e6455498               16.61   \n",
       "22937  5a5a4a1319679a5e560257abd2fa1273               12.83   \n",
       "18282  48251a875839d15f52e7dcb61e93628a               10.34   \n",
       "...                                 ...                 ...   \n",
       "39984  9e5fd9b33580d845c54855ee2e3965b0               15.15   \n",
       "63102  f8b730c3feb3136010e89c9acd18cdad                5.30   \n",
       "12678  3273805a17a78c15811a6feb489662fd                9.06   \n",
       "26392  68233f255c4f267082b2fc19f6449977               26.54   \n",
       "28420  705c73692cee1d4b25042f00e09d7688                8.37   \n",
       "\n",
       "       Delivery Time Previsto (Antigo)  Delivery Time Previsto (Novo)  \n",
       "18527                            18.20                         9.9720  \n",
       "49324                            11.50                         7.6606  \n",
       "43893                            41.01                        16.7375  \n",
       "22937                            25.20                        12.8334  \n",
       "18282                            32.35                        10.8214  \n",
       "...                                ...                            ...  \n",
       "39984                            28.38                        10.6027  \n",
       "63102                            17.32                         5.0609  \n",
       "12678                            27.33                         9.4697  \n",
       "26392                            21.96                        11.8614  \n",
       "28420                            19.64                         7.7572  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 215.1133719944513\n",
      "MAE: 13.011396424167694\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o modelo que recebemos na AWS\n",
    "mse = mean_squared_error(y_test, X_test['delivery_time_model'])\n",
    "mae = mean_absolute_error(y_test, X_test['delivery_time_model'])\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para imprimir a data específica que estamos prevendo que o pedido será entregue, mas como não precisamos disso no kaggle estou deixando de lado por enquanto\n",
    "\n",
    "'PrevisaoEntrega': [X_test['order_purchase_timestamp'].iloc[i] + timedelta(days=pred) for i, pred in enumerate(y_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Preencher valores ausentes com a mediana das colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo com regressor RandomForestRegressor\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Definir a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Treinar o modelo com GridSearch\n",
    "grid_search.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']), y_train)\n",
    "\n",
    "# Melhor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = best_model.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Diagnóstico para verificar as previsões\n",
    "print(\"Valores de previsão extremos:\", y_pred[(y_pred < 0) | (y_pred > 365)]) \n",
    "\n",
    "# Limitar valores de previsão a um intervalo razoável, por exemplo, 0 a 365 dias (1 ano)\n",
    "y_pred = np.clip(y_pred, 0, 365)\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'Melhores hiperparâmetros: {grid_search.best_params_}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(best_model, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('Data/resultado_previsoes.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões salvo com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
>>>>>>> 2126231c41efdf0f0a1ccd451ea116b8825667fd
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV de previsões para o kaggle salvo com sucesso!\n",
      "                           order_id  order_metric_cycle_time\n",
      "0  cee3292f46ede6ea1dfabfcb200fcf47                   9.5672\n",
      "1  50dca53ca33b739bef09e7933e8b380e                  10.2296\n",
      "2  8087ec71e393d4dc6fc48041fe63cd51                   5.8191\n",
      "3  e6b6557ce111de79b31cc857f20ba212                  10.3058\n",
      "4  0b09c5e4c2512f627190ac55a78c35a3                  14.2867\n"
     ]
    }
   ],
   "source": [
    "# código que retora o csv do jeito que ele precisa ser submetido no kaggle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Função para pré-processamento dos dados\n",
    "def preprocess_data(df):\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "    df['product_volume'] = df['product_length_cm_x'] * df['product_height_cm_x'] * df['product_width_cm_x']\n",
    "    df['product_weight_g'] = df['product_weight_g_x']\n",
    "    df = extract_datetime_features(df)\n",
    "    df = calculate_zip_similarity(df)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model_pipeline = joblib.load('model_pipeline.pkl')\n",
    "\n",
    "# Carregar os novos dados\n",
    "df_new = pd.read_csv(\"Data/kaggle_enriquecido.csv\")\n",
    "df_new = preprocess_data(df_new)\n",
    "\n",
    "# Variáveis preditoras\n",
    "X_new = df_new[['order_id', 'order_purchase_timestamp', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "\n",
    "# Fazer previsões nos novos dados\n",
    "y_pred_new = model_pipeline.predict(X_new.drop(columns=['order_id', 'order_purchase_timestamp']))\n",
    "\n",
    "# Criar DataFrame com as previsões\n",
    "df_resultado_new = pd.DataFrame({\n",
    "    'order_id': X_new['order_id'].values,\n",
    "    'order_metric_cycle_time': y_pred_new\n",
    "})\n",
    "\n",
    "# Salvar resultado como CSV sem cabeçalhos\n",
    "df_resultado_new.to_csv('Data/resultado_previsoes_kaggle2.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões para o kaggle salvo com sucesso!\")\n",
    "print(df_resultado_new.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28152.000000\n",
       "mean        10.936476\n",
       "std          3.431379\n",
       "min          2.430900\n",
       "25%          8.398900\n",
       "50%         11.341250\n",
       "75%         13.487275\n",
       "max         22.471000\n",
       "Name: Delivery Time Previsão, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado_new = pd.read_csv('Data/resultado_previsoes_kaggle.csv')\n",
    "\n",
    "# Descrever a coluna 'Delivery Time Previsão'\n",
    "delivery_time_stats = df_resultado_new['Delivery Time Previsão'].describe()\n",
    "\n",
    "delivery_time_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas na DataFrame final: 59352\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar a DataFrame principal e a DataFrame order_items\n",
    "df_principal = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "df_order_items = pd.read_csv(\"Data/order_items.csv\")\n",
    "\n",
    "# Corrigir zip codes\n",
    "df_principal['seller_zip_code_prefix'] = df_principal['seller_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n",
    "df_principal['customer_zip_code_prefix'] = df_principal['customer_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n",
    "\n",
    "# Remover linhas duplicadas da DataFrame order_items com base na coluna order_id\n",
    "df_order_items_clean = df_order_items.drop_duplicates(subset=['order_id'])\n",
    "\n",
    "# Realizar o merge usando pandas\n",
    "df_final = pd.merge(df_principal, df_order_items_clean, on='order_id', how='left')\n",
    "\n",
    "# Verificar o tamanho da DataFrame final\n",
    "print(f\"Número de linhas na DataFrame final: {len(df_final)}\")\n",
    "\n",
    "df_final.to_csv(\"Data/DataFrame_final_com_juncao.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18843.643010133725\n"
     ]
    }
   ],
   "source": [
    "# código que rodou por 13 horas procurando hiper parametros e retornou um mse quase igual\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Função para treinar um modelo com um conjunto de parâmetros\n",
    "def train_model(params):\n",
    "    model_pipeline.set_params(**params)\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    return model_pipeline\n",
    "\n",
    "# Usar ThreadPoolExecutor para paralelizar GridSearchCV\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(best_model, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "\n",
    "df_novo = extract_datetime_features(df)\n",
    "\n",
    "X_novo = df_novo[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "previsoes = modelo_carregado.predict(X_novo)\n",
    "\n",
    "df_novo['previsao_data_entrega_horas'] = previsoes\n",
    "df_novo['previsao_data_entrega'] = df_novo['order_purchase_timestamp'] + pd.to_timedelta(df_novo['previsao_data_entrega_horas'], unit='h')\n",
    "\n",
    "print(df_novo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
