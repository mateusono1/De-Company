{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Blue;\">Bibliotecas necessárias</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliotecas básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#treino, teste e avaliação\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#Implementação de modelos\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "#Tratamento e pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#API do google\n",
    "import csv\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Blue;\">Limpeza da base de dados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Pipeline de limpeza</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo duplicatas do DataFrame\n",
      "Tratamento de duplicatas concluído. Foram removidas 0 linhas.\n",
      "\n",
      "Removidas 35868 linhas devido a valores nulos nas colunas: order_approved_at, product_height_cm.\n",
      "\n",
      "Removendo colunas específicas do DataFrame\n",
      "Colunas removidas: ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'customer_city', 'customer_state']\n",
      "\n",
      "Convertendo coluna order_purchase_timestamp para datetime\n",
      "Convertendo coluna order_approved_at para datetime\n",
      "Convertendo coluna order_delivered_carrier_date para datetime\n",
      "Convertendo coluna order_delivered_customer_date para datetime\n",
      "\n",
      "Tratamento de números negativos concluído. Foram removidas 0 linhas devido à coluna 'Pedido em aprovação'.\n",
      "Tratamento de números negativos concluído. Foram removidas 435 linhas devido à coluna 'Separando o pedido'.\n",
      "Tratamento de números negativos concluído. Foram removidas 59 linhas devido à coluna 'Pedido em transporte'.\n",
      "\n",
      "Tratamento de outliers para coluna 'delivery_time' concluído. Foram removidas 3890 linhas.\n",
      "Tratamento de outliers para coluna 'Pedido em aprovação' concluído. Foram removidas 8075 linhas.\n",
      "Tratamento de outliers para coluna 'Separando o pedido' concluído. Foram removidas 2996 linhas.\n",
      "Tratamento de outliers para coluna 'Pedido em transporte' concluído. Foram removidas 930 linhas.\n",
      "\n",
      "Agrupamento de categorias concluído. Foram modificadas 15624 linhas.\n",
      "Ajuste de preço e valor do frete concluído.\n",
      "Correção dos códigos ZIP concluída.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>Pedido em aprovação</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>Separando o pedido</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>Pedido em transporte</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>delivery_time</th>\n",
       "      <th>delivery_time_model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>order_id.1</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>customer_id.1</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>order_id.2</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id.1</th>\n",
       "      <th>seller_id.1</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>3ce436f183e68e07877b285a838db11a</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-09-13 08:59:02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2017-09-13 09:45:35</td>\n",
       "      <td>6.37</td>\n",
       "      <td>2017-09-19 18:34:16</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2017-09-20 23:43:48</td>\n",
       "      <td>7.61</td>\n",
       "      <td>15.63</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>cool_stuff</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>27277000</td>\n",
       "      <td>volta redonda</td>\n",
       "      <td>SP</td>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>2.0</td>\n",
       "      <td>72.19</td>\n",
       "      <td>3ce436f183e68e07877b285a838db11a</td>\n",
       "      <td>871766c5855e863f6eccc05f988b23cb</td>\n",
       "      <td>28013.0000</td>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35.000</td>\n",
       "      <td>58.90</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>f6dd3ec061db4e3987629fe6b26e5cce</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-04-26 10:53:06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2017-04-26 11:05:13</td>\n",
       "      <td>8.15</td>\n",
       "      <td>2017-05-04 14:35:00</td>\n",
       "      <td>8.06</td>\n",
       "      <td>2017-05-12 16:04:24</td>\n",
       "      <td>16.22</td>\n",
       "      <td>18.55</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>pet_shop</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>03471000</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>3.0</td>\n",
       "      <td>259.83</td>\n",
       "      <td>f6dd3ec061db4e3987629fe6b26e5cce</td>\n",
       "      <td>eb28e67c4c0b83846050ddfb8a35d051</td>\n",
       "      <td>15775.0000</td>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13.000</td>\n",
       "      <td>239.90</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>6489ae5e4333f3693df5ad4372dab6d3</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-01-14 14:33:31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2018-01-14 14:48:30</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2018-01-16 12:36:48</td>\n",
       "      <td>6.03</td>\n",
       "      <td>2018-01-22 13:19:16</td>\n",
       "      <td>7.95</td>\n",
       "      <td>21.39</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>moveis</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>37564000</td>\n",
       "      <td>borda da mata</td>\n",
       "      <td>MG</td>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.87</td>\n",
       "      <td>6489ae5e4333f3693df5ad4372dab6d3</td>\n",
       "      <td>3818d81c6709e39d06b2738a8d3a2474</td>\n",
       "      <td>35661.0000</td>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30.000</td>\n",
       "      <td>199.00</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00048cc3ae777c65dbb7d2a0634bc1ea</td>\n",
       "      <td>816cbea969fe5b689b39cfc97a506742</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-05-15 21:42:34</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2017-05-17 03:55:27</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2017-05-17 11:05:55</td>\n",
       "      <td>5.11</td>\n",
       "      <td>2017-05-22 13:44:35</td>\n",
       "      <td>6.67</td>\n",
       "      <td>21.10</td>\n",
       "      <td>ef92defde845ab8450f9d70c526ef70f</td>\n",
       "      <td>utilidades_domesticas</td>\n",
       "      <td>450.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6426d21aca402a131fc0a5d0960a3c90</td>\n",
       "      <td>14091000</td>\n",
       "      <td>ribeirao preto</td>\n",
       "      <td>SP</td>\n",
       "      <td>00048cc3ae777c65dbb7d2a0634bc1ea</td>\n",
       "      <td>1.0</td>\n",
       "      <td>boleto</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.59</td>\n",
       "      <td>816cbea969fe5b689b39cfc97a506742</td>\n",
       "      <td>85c835d128beae5b4ce8602c491bf385</td>\n",
       "      <td>38017.0000</td>\n",
       "      <td>00048cc3ae777c65dbb7d2a0634bc1ea</td>\n",
       "      <td>1</td>\n",
       "      <td>ef92defde845ab8450f9d70c526ef70f</td>\n",
       "      <td>6426d21aca402a131fc0a5d0960a3c90</td>\n",
       "      <td>2017-05-23 03:55:27.000</td>\n",
       "      <td>21.90</td>\n",
       "      <td>12.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00054e8431b9d7675808bcb819fb4a32</td>\n",
       "      <td>32e2e6ab09e778d99bf2e0ecd4898718</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-12-10 11:53:48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2017-12-10 12:10:31</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2017-12-12 01:07:48</td>\n",
       "      <td>6.87</td>\n",
       "      <td>2017-12-18 22:03:38</td>\n",
       "      <td>8.42</td>\n",
       "      <td>24.50</td>\n",
       "      <td>8d4f2bb7e93e6710a28f34fa83ee7d28</td>\n",
       "      <td>telefonia</td>\n",
       "      <td>200.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7040e82f899a04d1b434b795a43b4617</td>\n",
       "      <td>01026000</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>00054e8431b9d7675808bcb819fb4a32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.75</td>\n",
       "      <td>32e2e6ab09e778d99bf2e0ecd4898718</td>\n",
       "      <td>635d9ac1680f03288e72ada3a1035803</td>\n",
       "      <td>16700.0000</td>\n",
       "      <td>00054e8431b9d7675808bcb819fb4a32</td>\n",
       "      <td>1</td>\n",
       "      <td>8d4f2bb7e93e6710a28f34fa83ee7d28</td>\n",
       "      <td>7040e82f899a04d1b434b795a43b4617</td>\n",
       "      <td>2017-12-14 12:10:31.000</td>\n",
       "      <td>19.90</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117589</th>\n",
       "      <td>fff8287bbae429a99bb7e8c21d151c41</td>\n",
       "      <td>6c1e92a209dbf868706caa831090941e</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-03-17 12:11:45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2018-03-17 12:29:22</td>\n",
       "      <td>4.29</td>\n",
       "      <td>2018-03-21 19:22:25</td>\n",
       "      <td>16.61</td>\n",
       "      <td>2018-04-07 10:07:48</td>\n",
       "      <td>20.91</td>\n",
       "      <td>32.49</td>\n",
       "      <td>bee2e070c39f3dd2f6883a17a5f0da45</td>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>175.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4e922959ae960d389249c378d1c939f5</td>\n",
       "      <td>12327000</td>\n",
       "      <td>jacarei</td>\n",
       "      <td>SP</td>\n",
       "      <td>fff8287bbae429a99bb7e8c21d151c41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>4.0</td>\n",
       "      <td>456.28</td>\n",
       "      <td>6c1e92a209dbf868706caa831090941e</td>\n",
       "      <td>028c09f007292c4e3a3b10d296e47987</td>\n",
       "      <td>58075.0000</td>\n",
       "      <td>fff8287bbae429a99bb7e8c21d151c41</td>\n",
       "      <td>2</td>\n",
       "      <td>bee2e070c39f3dd2f6883a17a5f0da45</td>\n",
       "      <td>4e922959ae960d389249c378d1c939f5</td>\n",
       "      <td>2018-03-27 12:29:22.000</td>\n",
       "      <td>180.00</td>\n",
       "      <td>48.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117590</th>\n",
       "      <td>fff90cdcb3b2e6cfb397d05d562fd3fe</td>\n",
       "      <td>f6cc7b845fde9d4e71361fe6fcd7ef75</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-24 09:03:47</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2017-11-24 10:11:58</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2017-11-27 22:44:45</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2017-11-29 02:24:25</td>\n",
       "      <td>4.72</td>\n",
       "      <td>13.62</td>\n",
       "      <td>764292b2b0f73f77a0272be03fdd45f3</td>\n",
       "      <td>moveis</td>\n",
       "      <td>750.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>bd23da7354813347129d751591d1a6e2</td>\n",
       "      <td>03971000</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>fff90cdcb3b2e6cfb397d05d562fd3fe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.73</td>\n",
       "      <td>f6cc7b845fde9d4e71361fe6fcd7ef75</td>\n",
       "      <td>0e1dad535a5b2359a2ff0a7d475ffb86</td>\n",
       "      <td>4119.0000</td>\n",
       "      <td>fff90cdcb3b2e6cfb397d05d562fd3fe</td>\n",
       "      <td>1</td>\n",
       "      <td>764292b2b0f73f77a0272be03fdd45f3</td>\n",
       "      <td>bd23da7354813347129d751591d1a6e2</td>\n",
       "      <td>2017-11-30 10:11:28.000</td>\n",
       "      <td>89.90</td>\n",
       "      <td>11.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117591</th>\n",
       "      <td>fffa82886406ccf10c7b4e35c4ff2788</td>\n",
       "      <td>a5201e1a6d71a8d21e869151bd5b4085</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-12-18 16:33:07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2017-12-18 17:33:04</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2017-12-20 18:55:00</td>\n",
       "      <td>18.98</td>\n",
       "      <td>2018-01-08 18:23:10</td>\n",
       "      <td>21.08</td>\n",
       "      <td>36.31</td>\n",
       "      <td>bbe7651fef80287a816ead73f065fc4b</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8f2ce03f928b567e3d56181ae20ae952</td>\n",
       "      <td>05141000</td>\n",
       "      <td>pirituba</td>\n",
       "      <td>SP</td>\n",
       "      <td>fffa82886406ccf10c7b4e35c4ff2788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>7.0</td>\n",
       "      <td>273.92</td>\n",
       "      <td>a5201e1a6d71a8d21e869151bd5b4085</td>\n",
       "      <td>2a3ab9bf9639491997586882c502540a</td>\n",
       "      <td>59955.0000</td>\n",
       "      <td>fffa82886406ccf10c7b4e35c4ff2788</td>\n",
       "      <td>1</td>\n",
       "      <td>bbe7651fef80287a816ead73f065fc4b</td>\n",
       "      <td>8f2ce03f928b567e3d56181ae20ae952</td>\n",
       "      <td>2017-12-22 17:31:42.000</td>\n",
       "      <td>229.90</td>\n",
       "      <td>44.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117601</th>\n",
       "      <td>fffce4705a9662cd70adb13d4a31832d</td>\n",
       "      <td>29309aa813182aaddc9b259e31b870e6</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-23 17:07:56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2017-10-24 17:14:25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2017-10-26 15:13:14</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2017-10-28 12:22:22</td>\n",
       "      <td>4.80</td>\n",
       "      <td>17.29</td>\n",
       "      <td>72a30483855e2eafc67aee5dc2560482</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>967.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>c3cfdc648177fdbbbb35635a37472c53</td>\n",
       "      <td>80610000</td>\n",
       "      <td>curitiba</td>\n",
       "      <td>PR</td>\n",
       "      <td>fffce4705a9662cd70adb13d4a31832d</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>3.0</td>\n",
       "      <td>116.85</td>\n",
       "      <td>29309aa813182aaddc9b259e31b870e6</td>\n",
       "      <td>cd79b407828f02fdbba457111c38e4c4</td>\n",
       "      <td>4039.0000</td>\n",
       "      <td>fffce4705a9662cd70adb13d4a31832d</td>\n",
       "      <td>1</td>\n",
       "      <td>72a30483855e2eafc67aee5dc2560482</td>\n",
       "      <td>c3cfdc648177fdbbbb35635a37472c53</td>\n",
       "      <td>2017-10-30 17:14:25.000</td>\n",
       "      <td>99.90</td>\n",
       "      <td>16.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117602</th>\n",
       "      <td>fffe18544ffabc95dfada21779c9644f</td>\n",
       "      <td>b5e6afd5a41800fdf401e0272ca74655</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-08-14 23:02:59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2017-08-15 00:04:32</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2017-08-15 19:02:53</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2017-08-16 21:59:40</td>\n",
       "      <td>1.96</td>\n",
       "      <td>10.04</td>\n",
       "      <td>9c422a519119dcad7575db5af1ba540e</td>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2b3e4a2a3ea8e01938cabda2a3e5cc79</td>\n",
       "      <td>04733000</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>fffe18544ffabc95dfada21779c9644f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.71</td>\n",
       "      <td>b5e6afd5a41800fdf401e0272ca74655</td>\n",
       "      <td>eb803377c9315b564bdedad672039306</td>\n",
       "      <td>13289.0000</td>\n",
       "      <td>fffe18544ffabc95dfada21779c9644f</td>\n",
       "      <td>1</td>\n",
       "      <td>9c422a519119dcad7575db5af1ba540e</td>\n",
       "      <td>2b3e4a2a3ea8e01938cabda2a3e5cc79</td>\n",
       "      <td>2017-08-21 00:04:32.000</td>\n",
       "      <td>55.99</td>\n",
       "      <td>8.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65351 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                order_id                       customer_id  \\\n",
       "0       00010242fe8c5a6d1ba2dd792cb16214  3ce436f183e68e07877b285a838db11a   \n",
       "1       00018f77f2f0320c557190d7a144bdd3  f6dd3ec061db4e3987629fe6b26e5cce   \n",
       "2       000229ec398224ef6ca0657da4fc703e  6489ae5e4333f3693df5ad4372dab6d3   \n",
       "5       00048cc3ae777c65dbb7d2a0634bc1ea  816cbea969fe5b689b39cfc97a506742   \n",
       "6       00054e8431b9d7675808bcb819fb4a32  32e2e6ab09e778d99bf2e0ecd4898718   \n",
       "...                                  ...                               ...   \n",
       "117589  fff8287bbae429a99bb7e8c21d151c41  6c1e92a209dbf868706caa831090941e   \n",
       "117590  fff90cdcb3b2e6cfb397d05d562fd3fe  f6cc7b845fde9d4e71361fe6fcd7ef75   \n",
       "117591  fffa82886406ccf10c7b4e35c4ff2788  a5201e1a6d71a8d21e869151bd5b4085   \n",
       "117601  fffce4705a9662cd70adb13d4a31832d  29309aa813182aaddc9b259e31b870e6   \n",
       "117602  fffe18544ffabc95dfada21779c9644f  b5e6afd5a41800fdf401e0272ca74655   \n",
       "\n",
       "       order_status order_purchase_timestamp  Pedido em aprovação  \\\n",
       "0         delivered      2017-09-13 08:59:02                 0.03   \n",
       "1         delivered      2017-04-26 10:53:06                 0.01   \n",
       "2         delivered      2018-01-14 14:33:31                 0.01   \n",
       "5         delivered      2017-05-15 21:42:34                 1.26   \n",
       "6         delivered      2017-12-10 11:53:48                 0.01   \n",
       "...             ...                      ...                  ...   \n",
       "117589    delivered      2018-03-17 12:11:45                 0.01   \n",
       "117590    delivered      2017-11-24 09:03:47                 0.05   \n",
       "117591    delivered      2017-12-18 16:33:07                 0.04   \n",
       "117601    delivered      2017-10-23 17:07:56                 1.00   \n",
       "117602    delivered      2017-08-14 23:02:59                 0.04   \n",
       "\n",
       "         order_approved_at  Separando o pedido order_delivered_carrier_date  \\\n",
       "0      2017-09-13 09:45:35                6.37          2017-09-19 18:34:16   \n",
       "1      2017-04-26 11:05:13                8.15          2017-05-04 14:35:00   \n",
       "2      2018-01-14 14:48:30                1.91          2018-01-16 12:36:48   \n",
       "5      2017-05-17 03:55:27                0.30          2017-05-17 11:05:55   \n",
       "6      2017-12-10 12:10:31                1.54          2017-12-12 01:07:48   \n",
       "...                    ...                 ...                          ...   \n",
       "117589 2018-03-17 12:29:22                4.29          2018-03-21 19:22:25   \n",
       "117590 2017-11-24 10:11:58                3.52          2017-11-27 22:44:45   \n",
       "117591 2017-12-18 17:33:04                2.06          2017-12-20 18:55:00   \n",
       "117601 2017-10-24 17:14:25                1.92          2017-10-26 15:13:14   \n",
       "117602 2017-08-15 00:04:32                0.79          2017-08-15 19:02:53   \n",
       "\n",
       "        Pedido em transporte order_delivered_customer_date  delivery_time  \\\n",
       "0                       1.21           2017-09-20 23:43:48           7.61   \n",
       "1                       8.06           2017-05-12 16:04:24          16.22   \n",
       "2                       6.03           2018-01-22 13:19:16           7.95   \n",
       "5                       5.11           2017-05-22 13:44:35           6.67   \n",
       "6                       6.87           2017-12-18 22:03:38           8.42   \n",
       "...                      ...                           ...            ...   \n",
       "117589                 16.61           2018-04-07 10:07:48          20.91   \n",
       "117590                  1.15           2017-11-29 02:24:25           4.72   \n",
       "117591                 18.98           2018-01-08 18:23:10          21.08   \n",
       "117601                  1.88           2017-10-28 12:22:22           4.80   \n",
       "117602                  1.12           2017-08-16 21:59:40           1.96   \n",
       "\n",
       "        delivery_time_model                        product_id  \\\n",
       "0                     15.63  4244733e06e7ecb4970a6e2683c13e61   \n",
       "1                     18.55  e5f2d52b802189ee658865ca93d83a8f   \n",
       "2                     21.39  c777355d18b72b67abbeef9df44fd0fd   \n",
       "5                     21.10  ef92defde845ab8450f9d70c526ef70f   \n",
       "6                     24.50  8d4f2bb7e93e6710a28f34fa83ee7d28   \n",
       "...                     ...                               ...   \n",
       "117589                32.49  bee2e070c39f3dd2f6883a17a5f0da45   \n",
       "117590                13.62  764292b2b0f73f77a0272be03fdd45f3   \n",
       "117591                36.31  bbe7651fef80287a816ead73f065fc4b   \n",
       "117601                17.29  72a30483855e2eafc67aee5dc2560482   \n",
       "117602                10.04  9c422a519119dcad7575db5af1ba540e   \n",
       "\n",
       "         product_category_name  product_weight_g  product_length_cm  \\\n",
       "0                   cool_stuff             650.0               28.0   \n",
       "1                     pet_shop           30000.0               50.0   \n",
       "2                       moveis            3050.0               33.0   \n",
       "5        utilidades_domesticas             450.0               24.0   \n",
       "6                    telefonia             200.0               27.0   \n",
       "...                        ...               ...                ...   \n",
       "117589  informatica_acessorios             175.0               20.0   \n",
       "117590                  moveis             750.0               30.0   \n",
       "117591           esporte_lazer            2700.0               60.0   \n",
       "117601           esporte_lazer             967.0               21.0   \n",
       "117602  informatica_acessorios             100.0               20.0   \n",
       "\n",
       "        product_height_cm  product_width_cm                         seller_id  \\\n",
       "0                     9.0              14.0  48436dade18ac8b2bce089ec2a041202   \n",
       "1                    30.0              40.0  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2                    13.0              33.0  5b51032eddd242adc84c38acab88f23d   \n",
       "5                     8.0              15.0  6426d21aca402a131fc0a5d0960a3c90   \n",
       "6                     5.0              20.0  7040e82f899a04d1b434b795a43b4617   \n",
       "...                   ...               ...                               ...   \n",
       "117589               20.0              20.0  4e922959ae960d389249c378d1c939f5   \n",
       "117590               30.0              11.0  bd23da7354813347129d751591d1a6e2   \n",
       "117591               15.0              15.0  8f2ce03f928b567e3d56181ae20ae952   \n",
       "117601               24.0              19.0  c3cfdc648177fdbbbb35635a37472c53   \n",
       "117602               20.0              20.0  2b3e4a2a3ea8e01938cabda2a3e5cc79   \n",
       "\n",
       "       seller_zip_code_prefix     seller_city seller_state  \\\n",
       "0                    27277000   volta redonda           SP   \n",
       "1                    03471000       sao paulo           SP   \n",
       "2                    37564000   borda da mata           MG   \n",
       "5                    14091000  ribeirao preto           SP   \n",
       "6                    01026000       sao paulo           SP   \n",
       "...                       ...             ...          ...   \n",
       "117589               12327000         jacarei           SP   \n",
       "117590               03971000       sao paulo           SP   \n",
       "117591               05141000        pirituba           SP   \n",
       "117601               80610000        curitiba           PR   \n",
       "117602               04733000       sao paulo           SP   \n",
       "\n",
       "                              order_id.1  payment_sequential payment_type  \\\n",
       "0       00010242fe8c5a6d1ba2dd792cb16214                 1.0  credit_card   \n",
       "1       00018f77f2f0320c557190d7a144bdd3                 1.0  credit_card   \n",
       "2       000229ec398224ef6ca0657da4fc703e                 1.0  credit_card   \n",
       "5       00048cc3ae777c65dbb7d2a0634bc1ea                 1.0       boleto   \n",
       "6       00054e8431b9d7675808bcb819fb4a32                 1.0  credit_card   \n",
       "...                                  ...                 ...          ...   \n",
       "117589  fff8287bbae429a99bb7e8c21d151c41                 1.0  credit_card   \n",
       "117590  fff90cdcb3b2e6cfb397d05d562fd3fe                 1.0  credit_card   \n",
       "117591  fffa82886406ccf10c7b4e35c4ff2788                 1.0  credit_card   \n",
       "117601  fffce4705a9662cd70adb13d4a31832d                 1.0  credit_card   \n",
       "117602  fffe18544ffabc95dfada21779c9644f                 1.0  credit_card   \n",
       "\n",
       "        payment_installments  payment_value                     customer_id.1  \\\n",
       "0                        2.0          72.19  3ce436f183e68e07877b285a838db11a   \n",
       "1                        3.0         259.83  f6dd3ec061db4e3987629fe6b26e5cce   \n",
       "2                        5.0         216.87  6489ae5e4333f3693df5ad4372dab6d3   \n",
       "5                        1.0          34.59  816cbea969fe5b689b39cfc97a506742   \n",
       "6                        1.0          31.75  32e2e6ab09e778d99bf2e0ecd4898718   \n",
       "...                      ...            ...                               ...   \n",
       "117589                   4.0         456.28  6c1e92a209dbf868706caa831090941e   \n",
       "117590                   1.0         101.73  f6cc7b845fde9d4e71361fe6fcd7ef75   \n",
       "117591                   7.0         273.92  a5201e1a6d71a8d21e869151bd5b4085   \n",
       "117601                   3.0         116.85  29309aa813182aaddc9b259e31b870e6   \n",
       "117602                   3.0          64.71  b5e6afd5a41800fdf401e0272ca74655   \n",
       "\n",
       "                      customer_unique_id customer_zip_code_prefix  \\\n",
       "0       871766c5855e863f6eccc05f988b23cb               28013.0000   \n",
       "1       eb28e67c4c0b83846050ddfb8a35d051               15775.0000   \n",
       "2       3818d81c6709e39d06b2738a8d3a2474               35661.0000   \n",
       "5       85c835d128beae5b4ce8602c491bf385               38017.0000   \n",
       "6       635d9ac1680f03288e72ada3a1035803               16700.0000   \n",
       "...                                  ...                      ...   \n",
       "117589  028c09f007292c4e3a3b10d296e47987               58075.0000   \n",
       "117590  0e1dad535a5b2359a2ff0a7d475ffb86                4119.0000   \n",
       "117591  2a3ab9bf9639491997586882c502540a               59955.0000   \n",
       "117601  cd79b407828f02fdbba457111c38e4c4                4039.0000   \n",
       "117602  eb803377c9315b564bdedad672039306               13289.0000   \n",
       "\n",
       "                              order_id.2  order_item_id  \\\n",
       "0       00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1       00018f77f2f0320c557190d7a144bdd3              1   \n",
       "2       000229ec398224ef6ca0657da4fc703e              1   \n",
       "5       00048cc3ae777c65dbb7d2a0634bc1ea              1   \n",
       "6       00054e8431b9d7675808bcb819fb4a32              1   \n",
       "...                                  ...            ...   \n",
       "117589  fff8287bbae429a99bb7e8c21d151c41              2   \n",
       "117590  fff90cdcb3b2e6cfb397d05d562fd3fe              1   \n",
       "117591  fffa82886406ccf10c7b4e35c4ff2788              1   \n",
       "117601  fffce4705a9662cd70adb13d4a31832d              1   \n",
       "117602  fffe18544ffabc95dfada21779c9644f              1   \n",
       "\n",
       "                            product_id.1                       seller_id.1  \\\n",
       "0       4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1       e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2       c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "5       ef92defde845ab8450f9d70c526ef70f  6426d21aca402a131fc0a5d0960a3c90   \n",
       "6       8d4f2bb7e93e6710a28f34fa83ee7d28  7040e82f899a04d1b434b795a43b4617   \n",
       "...                                  ...                               ...   \n",
       "117589  bee2e070c39f3dd2f6883a17a5f0da45  4e922959ae960d389249c378d1c939f5   \n",
       "117590  764292b2b0f73f77a0272be03fdd45f3  bd23da7354813347129d751591d1a6e2   \n",
       "117591  bbe7651fef80287a816ead73f065fc4b  8f2ce03f928b567e3d56181ae20ae952   \n",
       "117601  72a30483855e2eafc67aee5dc2560482  c3cfdc648177fdbbbb35635a37472c53   \n",
       "117602  9c422a519119dcad7575db5af1ba540e  2b3e4a2a3ea8e01938cabda2a3e5cc79   \n",
       "\n",
       "            shipping_limit_date   price  freight_value  \n",
       "0       2017-09-19 09:45:35.000   58.90          13.29  \n",
       "1       2017-05-03 11:05:13.000  239.90          19.93  \n",
       "2       2018-01-18 14:48:30.000  199.00          17.87  \n",
       "5       2017-05-23 03:55:27.000   21.90          12.69  \n",
       "6       2017-12-14 12:10:31.000   19.90          11.85  \n",
       "...                         ...     ...            ...  \n",
       "117589  2018-03-27 12:29:22.000  180.00          48.14  \n",
       "117590  2017-11-30 10:11:28.000   89.90          11.83  \n",
       "117591  2017-12-22 17:31:42.000  229.90          44.02  \n",
       "117601  2017-10-30 17:14:25.000   99.90          16.95  \n",
       "117602  2017-08-21 00:04:32.000   55.99           8.72  \n",
       "\n",
       "[65351 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Removendo linhas duplicadas\n",
    "\n",
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Removendo duplicatas do DataFrame\")\n",
    "        linhas_inicio = X.shape[0]\n",
    "        X_copy = X.copy()\n",
    "        X_copy.drop_duplicates(inplace=True)\n",
    "        linhas_fim = X_copy.shape[0]\n",
    "        print(f\"Tratamento de duplicatas concluído. Foram removidas {linhas_inicio - linhas_fim} linhas.\")\n",
    "        print()\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "# Removendo linhas em que o pedido foi cancelado ou que possuem produtos sem dimensão\n",
    "\n",
    "class RemoveInvalidRows(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns_to_check = [\"order_approved_at\", \"product_height_cm\"]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        initial_rows = X.shape[0]\n",
    "        X_copy = X.dropna(subset=self.columns_to_check)\n",
    "        removed_rows = initial_rows - X_copy.shape[0]\n",
    "        print(f\"Removidas {removed_rows} linhas devido a valores nulos nas colunas: {', '.join(self.columns_to_check)}.\")\n",
    "        print()\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Removendo colunas que não fornecem informações valiosas para o nosso modelo \n",
    "## (\"'order_id.1', 'customer_id.1', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty\")\n",
    "\n",
    "# Removendo colunas de localização do consumidor, pois possuem muitos dados faltantes, sendo que já temos as informações de zip code, suficientes para o nosso modelo\n",
    "## ('customer_city', 'customer_state')\n",
    "\n",
    "class CleanColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        columns_to_remove = [\n",
    "            'product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "            'customer_city', 'customer_state'\n",
    "        ]\n",
    "        print(\"Removendo colunas específicas do DataFrame\")\n",
    "        X_copy = X.copy()\n",
    "        X_copy.drop(columns=columns_to_remove, inplace=True)\n",
    "        print(f\"Colunas removidas: {columns_to_remove}\")\n",
    "        print()\n",
    "        return X_copy\n",
    "    \n",
    "# Convertendo colunas de data para a formatação DateTime\n",
    "\n",
    "class ConvertToDateTime(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = [\n",
    "            \"order_purchase_timestamp\",\n",
    "            \"order_approved_at\",\n",
    "            \"order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\"\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for column in self.columns:\n",
    "            if column in X_copy.columns:\n",
    "                print(f\"Convertendo coluna {column} para datetime\")\n",
    "                X_copy[column] = pd.to_datetime(X_copy[column], errors='coerce')\n",
    "        print()\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "# Criando novas colunas para analisar o intervalo de tempo dedicado a cada etapa do processo\n",
    "\n",
    "class AddTimeAnalysisColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # Calcular tempo de pedido em aprovação\n",
    "        PEDIDO_APROVACAO = X_copy[\"order_approved_at\"] - X_copy[\"order_purchase_timestamp\"]\n",
    "        PA2 = [(elem.total_seconds() / (24 * 3600)) if pd.notnull(elem) else None for elem in PEDIDO_APROVACAO]\n",
    "        PA2 = [round(elem, 2) if elem is not None else None for elem in PA2]\n",
    "\n",
    "        # Calcular tempo em que o pedido está sendo separado e enviado para a transportadora\n",
    "        SEPARANDO_PEDIDO = X_copy[\"order_delivered_carrier_date\"] - X_copy[\"order_approved_at\"]\n",
    "        SP2 = [(elem.total_seconds() / (24 * 3600)) if pd.notnull(elem) else None for elem in SEPARANDO_PEDIDO]\n",
    "        SP2 = [round(elem, 2) if elem is not None else None for elem in SP2]\n",
    "\n",
    "        # Calcular o tempo em que o pedido está em transporte até chegar na casa do cliente\n",
    "        PEDIDO_TRANSPORTE = X_copy[\"order_delivered_customer_date\"] - X_copy[\"order_delivered_carrier_date\"]\n",
    "        PT2 = [(elem.total_seconds() / (24 * 3600)) if pd.notnull(elem) else None for elem in PEDIDO_TRANSPORTE]\n",
    "        PT2 = [round(elem, 2) if elem is not None else None for elem in PT2]\n",
    "\n",
    "        # Inserir novas colunas no DataFrame\n",
    "        X_copy.insert(4, \"Pedido em aprovação\", PA2)\n",
    "        X_copy.insert(6, \"Separando o pedido\", SP2)\n",
    "        X_copy.insert(8, \"Pedido em transporte\", PT2)\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "# Removendo os casos em que o intervalo de tempo de determinada etapa é negativo\n",
    "\n",
    "class RemoveNegatives(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns_to_treat = [\"Pedido em aprovação\", \"Separando o pedido\", \"Pedido em transporte\"]\n",
    "        self.linhas_removidas = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for coluna in self.columns_to_treat:\n",
    "            linhas_inicio = X_copy.shape[0]\n",
    "            X_copy = X_copy[X_copy[coluna] >= 0]  \n",
    "            self.linhas_removidas[coluna] = linhas_inicio - X_copy.shape[0]\n",
    "        \n",
    "        for coluna, num_linhas in self.linhas_removidas.items():\n",
    "            print(f\"Tratamento de números negativos concluído. Foram removidas {num_linhas} linhas devido à coluna '{coluna}'.\")\n",
    "        print()\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "# Retirando outliers superiores\n",
    "\n",
    "class OutliersTreatment(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns_to_treat = [\"delivery_time\", \"Pedido em aprovação\", \"Separando o pedido\", \"Pedido em transporte\"]\n",
    "        self.outliers_limits = {}\n",
    "        self.removed_lines = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for coluna in self.columns_to_treat:\n",
    "            q1 = np.percentile(X[coluna], 25)\n",
    "            q3 = np.percentile(X[coluna], 75)\n",
    "            iqr = q3 - q1\n",
    "            out_sup = q3 + iqr * 1.5\n",
    "            out_inf = q1 - iqr * 1.5\n",
    "            self.outliers_limits[coluna] = (out_inf, out_sup)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        self.removed_lines = {}\n",
    "        for coluna in self.columns_to_treat:\n",
    "            out_inf, out_sup = self.outliers_limits[coluna]\n",
    "            initial_rows = X_copy.shape[0]\n",
    "            X_copy = X_copy[X_copy[coluna] < out_sup]\n",
    "            removed = initial_rows - X_copy.shape[0]\n",
    "            self.removed_lines[coluna] = removed\n",
    "            print(f\"Tratamento de outliers para coluna '{coluna}' concluído. Foram removidas {removed} linhas.\")\n",
    "        print()\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Agrupando categorias de produtos \n",
    "\n",
    "class AgruparCategorias(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.categorias_a_agrupar = {\n",
    "            'construcao_ferramentas_ferramentas': 'construcao_ferramentas',\n",
    "            'construcao_ferramentas_construcao': 'construcao_ferramentas',\n",
    "            'construcao_ferramentas_jardim': 'construcao_ferramentas',\n",
    "            'construcao_ferramentas_iluminacao': 'construcao_ferramentas',\n",
    "            'construcao_ferramentas_seguranca': 'construcao_ferramentas',\n",
    "            'ferramentas_jardim': 'construcao_ferramentas',\n",
    "\n",
    "            'moveis_sala': 'moveis',\n",
    "            'moveis_quarto': 'moveis',\n",
    "            'moveis_colchao_e_estofado': 'moveis',\n",
    "            'moveis_cozinha_area_de_servico_jantar_e_jardim': 'moveis',\n",
    "            'moveis_decoracao': 'moveis',\n",
    "            'moveis_escritorio': 'moveis',\n",
    "\n",
    "            'pc_gamer': 'pcs',\n",
    "\n",
    "            'artes_e_artesanato': 'artes',\n",
    "\n",
    "            'telefonia_fixa': 'telefonia',\n",
    "\n",
    "            'alimentos': 'alimentos_bebidas',\n",
    "            'bebidas': 'alimentos_bebidas',\n",
    "\n",
    "            'cds_dvds_musicais': 'cds_dvds',\n",
    "            'dvds_blu_ray': 'cds_dvds',\n",
    "\n",
    "            'portateis_casa_forno_e_cafe': 'eletroportateis',\n",
    "\n",
    "            'casa_conforto_2': 'casa_conforto',\n",
    "\n",
    "            'eletrodomesticos_2': 'eletrodomesticos',\n",
    "\n",
    "            'malas_acessorios': 'fashion',\n",
    "            'fashion_bolsas_e_acessorios': 'fashion',\n",
    "            'fashion_calcados': 'fashion',\n",
    "            'fashion_underwear_e_moda_praia': 'fashion',\n",
    "            'fashion_roupa_masculina': 'fashion',\n",
    "            'fashion_esporte': 'fashion',\n",
    "            'fashion_roupa_feminina': 'fashion',\n",
    "            'fashion_roupa_infanto_juvenil': 'fashion',\n",
    "\n",
    "            'eletronicos': 'informatica_acessorios',\n",
    "            'tablets_impressao_imagem': 'informatica_acessorios',\n",
    "\n",
    "            'la_cuisine': 'utilidades_domesticas',\n",
    "            \n",
    "            'fraldas_higiene': 'bebes'\n",
    "        }\n",
    "        self.num_modified = 0\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        modified_rows = X_copy['product_category_name'].replace(self.categorias_a_agrupar)\n",
    "        self.num_modified = (X_copy['product_category_name'] != modified_rows).sum()\n",
    "        X_copy['product_category_name'] = modified_rows\n",
    "        print(f\"Agrupamento de categorias concluído. Foram modificadas {self.num_modified} linhas.\")\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "class AjustandoPrecoEValorFrete(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, freight_cap=300):\n",
    "        self.freight_cap = freight_cap\n",
    "        self.freight_median = None\n",
    "        self.price_median = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcular a mediana dos valores de freight e price\n",
    "        self.freight_median = X['freight_value'].median()\n",
    "        self.price_median = X['price'].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Limitar o valor do frete\n",
    "        X_copy.loc[X_copy[\"freight_value\"] > self.freight_cap, \"freight_value\"] = self.freight_cap\n",
    "        \n",
    "        # Preencher valores nulos com a mediana calculada\n",
    "        X_copy['price'].fillna(self.price_median, inplace=True)\n",
    "        X_copy['freight_value'].fillna(self.freight_median, inplace=True)\n",
    "        \n",
    "        # Reaplicar a mediana para valores acima do limite após o primeiro ajuste\n",
    "        X_copy.loc[X_copy[\"freight_value\"] > self.freight_cap, \"freight_value\"] = self.freight_median\n",
    "        \n",
    "        print(\"Ajuste de preço e valor do frete concluído.\")\n",
    "        return X_copy\n",
    "\n",
    "# Ajustando para que todos os zip codes tenham 8 dígitos, necessário para quando formos buscar pelas distâncias e tempo de deslocamento\n",
    "\n",
    "class CorrigindoZipCodes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        X_copy['seller_zip_code_prefix'] = X_copy['seller_zip_code_prefix'].astype(str).apply(\n",
    "            lambda x: x.zfill(5) if len(x) == 4 else x) + '000'\n",
    "        X_copy['customer_zip_code_prefix'] = X_copy['customer_zip_code_prefix'].astype(str).apply(\n",
    "            lambda x: x.zfill(5) if len(x) == 4 else x) + '000'\n",
    "        \n",
    "        print(\"Correção dos códigos ZIP concluída.\")\n",
    "        return X_copy\n",
    "    \n",
    "pipeline_preprocessamento = Pipeline([\n",
    "    ('Remover duplicatas', RemoveDuplicates()),\n",
    "    ('Remover linhas nulas', RemoveInvalidRows()),\n",
    "    ('Tirar colunas desnecessárias', CleanColumns()),\n",
    "    ('Converter colunas para DateTime', ConvertToDateTime()),\n",
    "    ('Criando colunas com intervalos de tempo', AddTimeAnalysisColumns()),\n",
    "    ('Remover intervalos de tempo negativos', RemoveNegatives()),\n",
    "    ('Remover outliers', OutliersTreatment()),\n",
    "    ('Agrupar categorias de produtos', AgruparCategorias()),\n",
    "    ('Ajustar preço e valor do frete', AjustandoPrecoEValorFrete()),\n",
    "    ('Corrigir zip codes', CorrigindoZipCodes())\n",
    "])\n",
    "\n",
    "df = pd.read_csv(\"query.csv\")\n",
    "\n",
    "dados_preprocessados = pipeline_preprocessamento.fit_transform(df)\n",
    "\n",
    "display(dados_preprocessados)\n",
    "# Exportando o DataFrame para um arquivo CSV\n",
    "dados_preprocessados.to_csv(\"dataframe_limpa.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Aplicação da API do Google</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina sua chave de API do Google\n",
    "API_KEY = 'AIzaSyB5pCi6lgAW9Kq6b3w2tD1lh8vaXJsS6hc'\n",
    "\n",
    "# Defina o URL da API\n",
    "base_url = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n",
    "\n",
    "# Configurar sessão com retries\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Função para obter a distância e o tempo de viagem\n",
    "def get_distance_duration(origins, destinations):\n",
    "    params = {\n",
    "        'origins': origins,\n",
    "        'destinations': destinations,\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(base_url, params=params, timeout=10)\n",
    "        result = response.json()\n",
    "        \n",
    "        if result['status'] == 'OK':\n",
    "            row = result['rows'][0]\n",
    "            element = row['elements'][0]\n",
    "            if element['status'] == 'OK':\n",
    "                distance = element['distance']['value'] # Distância em metros\n",
    "                duration = element['duration']['value'] # Tempo de viagem em segundos\n",
    "                return distance, duration\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Arquivos de entrada e saída\n",
    "input_csv = 'dataframe_limpa.csv'\n",
    "output_csv = 'dataframe_limpa2.csv'\n",
    "\n",
    "# Função para processar cada linha do CSV\n",
    "def process_row(row):\n",
    "    seller_zip = row['seller_zip_code_prefix']\n",
    "    buyer_zip = row['customer_zip_code_prefix']\n",
    "    \n",
    "    distance, duration = get_distance_duration(seller_zip, buyer_zip)\n",
    "    \n",
    "    row['distance_meters'] = distance\n",
    "    row['duration_seconds'] = duration\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Abrindo os arquivos\n",
    "with open(input_csv, mode='r') as infile, open(output_csv, mode='w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + ['distance_meters', 'duration_seconds']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    rows_to_process = list(reader)\n",
    "    \n",
    "    # Utilizando ThreadPoolExecutor para processamento assíncrono\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_row, row) for row in rows_to_process]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            processed_row = future.result()\n",
    "            writer.writerow(processed_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Blue;\">Teste de Modelos</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalToCodes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.categories = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()  # Fazer uma cópia do DataFrame\n",
    "        for col in self.columns:\n",
    "            X[col] = pd.Categorical(X[col])\n",
    "            self.categories[col] = X[col].cat.categories\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()  # Fazer uma cópia do DataFrame\n",
    "        for col in self.columns:\n",
    "            X[col] = pd.Categorical(X[col], categories=self.categories[col])\n",
    "            X.loc[:, col] = X[col].cat.codes  # Usar .loc para evitar o aviso\n",
    "        return X\n",
    "\n",
    "# Lista de colunas a serem transformadas\n",
    "columns_to_transform = [\"customer_zip_code_prefix\", \"seller_zip_code_prefix\", \"product_category_name\"]\n",
    "\n",
    "# Seleção das colunas desejadas do DataFrame\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]\n",
    "\n",
    "# Colunas a serem selecionadas\n",
    "selected_columns = [\"product_category_name\", \"delivery_time\", \"product_weight_g\", \"product_length_cm\",\n",
    "                    \"product_height_cm\", \"product_width_cm\", \"seller_zip_code_prefix\",\n",
    "                    \"customer_zip_code_prefix\", \"price\", \"freight_value\"]\n",
    "\n",
    "# Criando o pipeline\n",
    "pipeline_1 = Pipeline([\n",
    "    ('column_selector', ColumnSelector(columns=selected_columns)),\n",
    "    ('categorical_to_codes', CategoricalToCodes(columns=columns_to_transform))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Multi Layer Perceptron</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Adequando a DataFrame a esse modelo em específico</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataframe_limpa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicando o pipeline\n",
    "df = pipeline_1.fit_transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Treinando e avaliando o modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:8.393693444195156\n",
      "MSE:70.45408963512473\n"
     ]
    }
   ],
   "source": [
    "x=df.drop(columns=\"delivery_time\")\n",
    "y=df[\"delivery_time\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=11)\n",
    "\n",
    "reg=MLPRegressor(hidden_layer_sizes=(5, 5), activation='relu', solver='adam', max_iter=1000)\n",
    "reg.fit(x_train,y_train)\n",
    "\n",
    "y_pred=reg.predict(x_test)\n",
    "\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE:{rmse}\")\n",
    "print(f\"MSE:{mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Linear Regression</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Adequando a DataFrame a esse modelo em específico</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataframe_limpa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pipeline_1.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Treinando e avaliando o modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:5.245036345562816\n",
      "MSE:27.51040626627494\n"
     ]
    }
   ],
   "source": [
    "x=df.drop(columns=[\"delivery_time\"])\n",
    "y=df[\"delivery_time\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "\n",
    "y_pred=lr.predict(x_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE:{rmse}\")\n",
    "print(f\"MSE:{mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Gradient Boost</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Adequando a DataFrame a esse modelo em específico</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataframe_limpa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pipeline_1.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Treinando e avaliando o modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:4.695866037947729\n",
      "MSE:22.051157846350904\n"
     ]
    }
   ],
   "source": [
    "x=df.drop(columns=[\"delivery_time\"])\n",
    "y=df[\"delivery_time\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "reg=GradientBoostingRegressor()\n",
    "reg.fit(x_train,y_train)\n",
    "\n",
    "y_pred=reg.predict(x_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE:{rmse}\")\n",
    "print(f\"MSE:{mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">XGBoost</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Adequando a DataFrame a esse modelo em específico</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataframe_limpa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pipeline_1.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Treinando e avaliando o modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=\"delivery_time\")\n",
    "y=df[\"delivery_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:4.509254935777832\n",
      "MSE:20.333380075836747\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=11)\n",
    "reg=xgb.XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "                     \n",
    "reg.fit(x_train,y_train)\n",
    "\n",
    "y_pred=reg.predict(x_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE:{rmse}\")\n",
    "print(f\"MSE:{mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Random Forest</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Adequando a DataFrame a esse modelo em específico</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melhor até agora, retorna direto o delivery time\n",
    "# retorna um csv resultado_previsoes que compara o nosso modelo com o antigo fornecido na AWS\n",
    "# usa o zip code como categorico e corrige os zip codes de sao paulo\n",
    "# utiliza o product_category_name, seller_city e seller_state\n",
    "\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"dataframe_limpa2.csv\")\n",
    "\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "\n",
    "# Extrair características de data/hora\n",
    "df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "\n",
    "# Preencher valores ausentes com a mediana das colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'approval_delay', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'product_volume', 'product_weight_g', 'seller_zip_code_prefix', 'customer_zip_code_prefix', 'freight_value', 'price', 'product_category_name', 'seller_city', 'seller_state', 'distance_meters']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Treinando e avaliando o modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18.55248494494559\n",
      "RMSE: 4.307259563219471\n"
     ]
    }
   ],
   "source": [
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'approval_delay', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'product_volume', 'product_weight_g', 'seller_zip_code_prefix', 'customer_zip_code_prefix', 'freight_value', 'price', 'product_category_name', 'seller_city', 'seller_state', 'distance_meters']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'freight_value', 'price', 'distance_meters']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['purchase_weekday', 'purchase_month', 'purchase_hour', 'seller_zip_code_prefix', 'customer_zip_code_prefix', 'product_category_name', 'seller_city', 'seller_state'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo com regressor RandomForestRegressor\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "model_pipeline.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp','delivery_time_model']), y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = model_pipeline.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('resultado_previsoes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Tunagem do modelo</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # tunando o random forest regressor\n",
    "# Definir a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Treinar o modelo com GridSearch\n",
    "grid_search.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']), y_train)\n",
    "\n",
    "# Melhor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = best_model.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Diagnóstico para verificar as previsões\n",
    "print(\"Valores de previsão extremos:\", y_pred[(y_pred < 0) | (y_pred > 365)]) \n",
    "\n",
    "# Limitar valores de previsão a um intervalo razoável, por exemplo, 0 a 365 dias (1 ano)\n",
    "y_pred = np.clip(y_pred, 0, 365)\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'Melhores hiperparâmetros: {grid_search.best_params_}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(best_model, 'tuned_model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('resultado_previsoes_tunado.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões salvo com sucesso!\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Blue;\">Aplicação do Random Forest na base de dados do Kaggle</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Enriquecimento da Base do Kaggle</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Coletando e tratando arquivos necessários</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=pd.read_csv(\"Enriquecimento do Kaggle/csv_Kaggle.csv\")\n",
    "#Atenção, esse arquivo - csv_Kaggle.csv - é o arquivo cru puxado do Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer=pd.read_csv(\"Enriquecimento do Kaggle/customers.csv\")\n",
    "order_items=pd.read_csv(\"Enriquecimento do Kaggle/order_items.csv\")\n",
    "products=pd.read_csv(\"Enriquecimento do Kaggle/products.csv\")\n",
    "sellers=pd.read_csv(\"Enriquecimento do Kaggle/sellers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer.drop_duplicates(inplace=True)\n",
    "order_items.drop_duplicates(inplace=True)\n",
    "products.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.drop([\"product_name_lenght\",\"product_description_lenght\",\"product_photos_qty\"],inplace=True,axis=1)\n",
    "order_items.drop(\"order_item_id\",axis=1, inplace=True)\n",
    "customer.drop(\"customer_unique_id\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Enriquecimento</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.drop_duplicates(subset=\"order_id\",inplace=True)\n",
    "df=kaggle.merge(order_items,how=\"left\",on=\"order_id\")\n",
    "df=df.merge(customer, how=\"left\", on=\"customer_id\")\n",
    "df=df.merge(products,how=\"left\",on=\"product_id\")\n",
    "df=df.merge(sellers,how=\"left\",on=\"seller_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Tratamento dos dados numéricos</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].fillna(df['price'].median(), inplace=True)\n",
    "df['freight_value'].fillna(df['freight_value'].median(), inplace=True)\n",
    "df.loc[df[\"freight_value\"]>300,\"freight_value\"]=df[\"freight_value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x) + '000'\n",
    "df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x) + '000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Tratamento dos dados categóricos</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias_a_agrupar = {\n",
    "    'construcao_ferramentas_ferramentas': 'construcao_ferramentas',\n",
    "    'construcao_ferramentas_construcao': 'construcao_ferramentas',\n",
    "    'construcao_ferramentas_jardim': 'construcao_ferramentas',\n",
    "    'construcao_ferramentas_iluminacao': 'construcao_ferramentas',\n",
    "    'construcao_ferramentas_seguranca': 'construcao_ferramentas',\n",
    "    'ferramentas_jardim': 'construcao_ferramentas',\n",
    "    'moveis_sala': 'moveis',\n",
    "    'moveis_quarto': 'moveis',\n",
    "    'moveis_colchao_e_estofado': 'moveis',\n",
    "    'moveis_cozinha_area_de_servico_jantar_e_jardim': 'moveis',\n",
    "    'moveis_decoracao': 'moveis',\n",
    "    'moveis_escritorio': 'moveis',\n",
    "    'pc_gamer': 'pcs',\n",
    "    'artes_e_artesanato':'artes',\n",
    "    'telefonia_fixa':'telefonia',\n",
    "    'alimentos': 'alimentos_bebidas',\n",
    "    'bebidas': 'alimentos_bebidas',\n",
    "    'cds_dvds_musicais': 'cds_dvds',\n",
    "    'dvds_blu_ray': 'cds_dvds',\n",
    "    'portateis_casa_forno_e_cafe': 'eletroportateis',\n",
    "    'casa_conforto_2': 'casa_conforto',\n",
    "    'eletrodomesticos_2': 'eletrodomesticos',\n",
    "    'malas_acessorios':'fashion',\n",
    "    'fashion_bolsas_e_acessorios':'fashion',\n",
    "    'fashion_calcados':'fashion',\n",
    "    'fashion_underwear_e_moda_praia':'fashion',\n",
    "    'fashion_roupa_masculina':'fashion',\n",
    "    'fashion_esporte':'fashion' ,\n",
    "    'fashion_roupa_feminina':'fashion',\n",
    "    'fashion_roupa_infanto_juvenil':'fashion',\n",
    "    'eletronicos':'informatica_acessorios',\n",
    "    'tablets_impressao_imagem':'informatica_acessorios',\n",
    "    'la_cuisine':'utilidades_domesticas',\n",
    "    'fraldas_higiene': 'bebes'\n",
    "    \n",
    "}\n",
    "\n",
    "df['product_category_name'].replace(categorias_a_agrupar, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Arquivos intermediários/kaggle_intermediario.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Aplicação do API do Google</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina sua chave de API do Google\n",
    "API_KEY = 'AIzaSyB5pCi6lgAW9Kq6b3w2tD1lh8vaXJsS6hc'\n",
    "\n",
    "# Defina o URL da API\n",
    "base_url = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n",
    "\n",
    "# Configurar sessão com retries\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Função para obter a distância e o tempo de viagem\n",
    "def get_distance_duration(origins, destinations):\n",
    "    params = {\n",
    "        'origins': origins,\n",
    "        'destinations': destinations,\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(base_url, params=params, timeout=10)\n",
    "        result = response.json()\n",
    "        \n",
    "        if result['status'] == 'OK':\n",
    "            row = result['rows'][0]\n",
    "            element = row['elements'][0]\n",
    "            if element['status'] == 'OK':\n",
    "                distance = element['distance']['value'] # Distância em metros\n",
    "                duration = element['duration']['value'] # Tempo de viagem em segundos\n",
    "                return distance, duration\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Arquivos de entrada e saída\n",
    "input_csv = 'Arquivos intermediários/kaggle_intermediario.csv'\n",
    "output_csv = 'kaggle_final.csv'\n",
    "\n",
    "# Função para processar cada linha do CSV\n",
    "def process_row(row):\n",
    "    seller_zip = row['seller_zip_code_prefix']\n",
    "    buyer_zip = row['customer_zip_code_prefix']\n",
    "    \n",
    "    distance, duration = get_distance_duration(seller_zip, buyer_zip)\n",
    "    \n",
    "    row['distance_meters'] = distance\n",
    "    row['duration_seconds'] = duration\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Abrindo os arquivos\n",
    "with open(input_csv, mode='r') as infile, open(output_csv, mode='w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + ['distance_meters', 'duration_seconds']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    rows_to_process = list(reader)\n",
    "    \n",
    "    # Utilizando ThreadPoolExecutor para processamento assíncrono\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_row, row) for row in rows_to_process]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            processed_row = future.result()\n",
    "            writer.writerow(processed_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: White;\">Complementando informações (tempo e distância) com a mediana</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"kaggle_final.csv\")\n",
    "\n",
    "df.loc[pd.isna(df[\"distance_meters\"]), \"distance_meters\"] = df[\"distance_meters\"].median()\n",
    "df.loc[pd.isna(df[\"duration_seconds\"]), \"duration_seconds\"] = df[\"duration_seconds\"].median()\n",
    "\n",
    "df.to_csv(\"kaggle_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #6fa8dc;\">Aplicação do Random Forest na Base do Kaggle</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tuned_model_pipeline.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[403], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Carregar o modelo treinado\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtuned_model_pipeline.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Carregar os novos dados\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe_limpa2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tuned_model_pipeline.pkl'"
     ]
    }
   ],
   "source": [
    "# codigo que retorna o csv do jeito que ele tem que ser enviado no kaggle\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para pré-processamento dos dados\n",
    "def preprocess_data(df):\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "    df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "    \n",
    "    # Extrair características de data/hora\n",
    "    df = extract_datetime_features(df)\n",
    "    \n",
    "    # Preencher valores ausentes com a mediana das colunas numéricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Função para garantir que os tipos de dados estão corretos\n",
    "def ensure_correct_types(df):\n",
    "    df['seller_zip_code_prefix'] = df['seller_zip_code_prefix'].astype(str)\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model_pipeline = joblib.load('tuned_model_pipeline.pkl')\n",
    "\n",
    "# Carregar os novos dados\n",
    "df_new = pd.read_csv(\"dataframe_limpa2.csv\")\n",
    "\n",
    "# Pré-processar os dados\n",
    "df_new = preprocess_data(df_new)\n",
    "df_new = ensure_correct_types(df_new)\n",
    "\n",
    "# Variáveis preditoras\n",
    "X_new = df_new[['order_id', 'order_purchase_timestamp', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'seller_zip_code_prefix', 'customer_zip_code_prefix', 'freight_value', 'price', 'product_category_name', 'seller_city', 'seller_state', 'distance_meters']]\n",
    "\n",
    "# Fazer previsões nos novos dados\n",
    "y_pred_new = model_pipeline.predict(X_new.drop(columns=['order_id', 'order_purchase_timestamp']))\n",
    "\n",
    "# Criar DataFrame com as previsões\n",
    "df_resultado_new = pd.DataFrame({\n",
    "    'order_id': X_new['order_id'].values,\n",
    "    'order_metric_cycle_time': y_pred_new\n",
    "})\n",
    "\n",
    "# Salvar resultado como CSV sem cabeçalhos\n",
    "df_resultado_new.to_csv('resultado_previsoes_kaggle5.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões para o Kaggle salvo com sucesso!\")\n",
    "print(df_resultado_new.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcular a data de entrega prevista\n",
    "previsao_entrega = X_new['order_purchase_timestamp'] + pd.to_timedelta(y_pred_new, unit='D')\n",
    "\n",
    "# Criar DataFrame com as previsões de datas\n",
    "df_resultado_datas = pd.DataFrame({\n",
    "    'order_id': X_new['order_id'].values,\n",
    "    'data de compra': X_new['order_purchase_timestamp'].values,\n",
    "    'data de entrega': previsao_entrega.values\n",
    "})\n",
    "\n",
    "# Salvar DataFrame com as datas de entrega previstas\n",
    "df_resultado_datas.to_csv('resultado_datas_entrega.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
