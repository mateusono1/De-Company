{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 52\u001b[0m\n\u001b[0;32m     46\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     47\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     48\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m     49\u001b[0m ])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Treinar o modelo\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Fazer previsões\u001b[39;00m\n\u001b[0;32m     55\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py:476\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    475\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 476\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# primeira tentativa com random forest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18521.300670029792\n",
      "MAE: 105.63585213014834\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18136.803027188955\n",
      "MAE: 104.82506590254518\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18007.406524008937\n",
      "MAE: 104.4635009544379\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18.415524473382906\n",
      "MAE: 3.155886588519814\n"
     ]
    }
   ],
   "source": [
    "# o melhor até agora, pois usou o freight_value, mas ta prevendo a data e nao o delivery time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Adicionar a variável de frete (freight_value)\n",
    "df['freight_value'] = df['freight_value']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Garantir que não haja valores NaN nos dados\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Usar ThreadPoolExecutor para treino do modelo\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(train_model, model_pipeline, X_train, y_train)\n",
    "    future.result()  # Esperar a conclusão do treino\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de previsão extremos: []\n",
      "MSE: 18.4067136434971\n",
      "MAE: 3.1528348047677763\n",
      "CSV de previsões salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# melhor até agora, retorna direto o delivery time\n",
    "# retorna um csv resultado_previsoes que compara o nosso modelo com o antigo fornecido na AWS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Adicionar a variável de frete (freight_value)\n",
    "df['freight_value'] = df['freight_value']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Preencher valores ausentes com a mediana das colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo com regressor RandomForestRegressor\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "model_pipeline.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp','delivery_time_model']), y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = model_pipeline.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Diagnóstico para verificar as previsões\n",
    "print(\"Valores de previsão extremos:\", y_pred[(y_pred < 0) | (y_pred > 365)]) \n",
    "\n",
    "# Limitar valores de previsão a um intervalo razoável, por exemplo, 0 a 365 dias (1 ano)\n",
    "y_pred = np.clip(y_pred, 0, 365)\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(model_pipeline, 'model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('Data/resultado_previsoes.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões salvo com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>Delivery Time Real</th>\n",
       "      <th>Delivery Time Previsto (Antigo)</th>\n",
       "      <th>Delivery Time Previsto (Novo)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18527</th>\n",
       "      <td>4912377c02eb84d181d53e98e010124e</td>\n",
       "      <td>8.81</td>\n",
       "      <td>18.20</td>\n",
       "      <td>9.9720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49324</th>\n",
       "      <td>c25acd72d5c2eb2ec89c3c8d7cd7d1f2</td>\n",
       "      <td>4.26</td>\n",
       "      <td>11.50</td>\n",
       "      <td>7.6606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43893</th>\n",
       "      <td>ad4c4ee12613fc3dc5d506e4e6455498</td>\n",
       "      <td>16.61</td>\n",
       "      <td>41.01</td>\n",
       "      <td>16.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22937</th>\n",
       "      <td>5a5a4a1319679a5e560257abd2fa1273</td>\n",
       "      <td>12.83</td>\n",
       "      <td>25.20</td>\n",
       "      <td>12.8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>48251a875839d15f52e7dcb61e93628a</td>\n",
       "      <td>10.34</td>\n",
       "      <td>32.35</td>\n",
       "      <td>10.8214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>9e5fd9b33580d845c54855ee2e3965b0</td>\n",
       "      <td>15.15</td>\n",
       "      <td>28.38</td>\n",
       "      <td>10.6027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63102</th>\n",
       "      <td>f8b730c3feb3136010e89c9acd18cdad</td>\n",
       "      <td>5.30</td>\n",
       "      <td>17.32</td>\n",
       "      <td>5.0609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12678</th>\n",
       "      <td>3273805a17a78c15811a6feb489662fd</td>\n",
       "      <td>9.06</td>\n",
       "      <td>27.33</td>\n",
       "      <td>9.4697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26392</th>\n",
       "      <td>68233f255c4f267082b2fc19f6449977</td>\n",
       "      <td>26.54</td>\n",
       "      <td>21.96</td>\n",
       "      <td>11.8614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28420</th>\n",
       "      <td>705c73692cee1d4b25042f00e09d7688</td>\n",
       "      <td>8.37</td>\n",
       "      <td>19.64</td>\n",
       "      <td>7.7572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                OrderID  Delivery Time Real  \\\n",
       "18527  4912377c02eb84d181d53e98e010124e                8.81   \n",
       "49324  c25acd72d5c2eb2ec89c3c8d7cd7d1f2                4.26   \n",
       "43893  ad4c4ee12613fc3dc5d506e4e6455498               16.61   \n",
       "22937  5a5a4a1319679a5e560257abd2fa1273               12.83   \n",
       "18282  48251a875839d15f52e7dcb61e93628a               10.34   \n",
       "...                                 ...                 ...   \n",
       "39984  9e5fd9b33580d845c54855ee2e3965b0               15.15   \n",
       "63102  f8b730c3feb3136010e89c9acd18cdad                5.30   \n",
       "12678  3273805a17a78c15811a6feb489662fd                9.06   \n",
       "26392  68233f255c4f267082b2fc19f6449977               26.54   \n",
       "28420  705c73692cee1d4b25042f00e09d7688                8.37   \n",
       "\n",
       "       Delivery Time Previsto (Antigo)  Delivery Time Previsto (Novo)  \n",
       "18527                            18.20                         9.9720  \n",
       "49324                            11.50                         7.6606  \n",
       "43893                            41.01                        16.7375  \n",
       "22937                            25.20                        12.8334  \n",
       "18282                            32.35                        10.8214  \n",
       "...                                ...                            ...  \n",
       "39984                            28.38                        10.6027  \n",
       "63102                            17.32                         5.0609  \n",
       "12678                            27.33                         9.4697  \n",
       "26392                            21.96                        11.8614  \n",
       "28420                            19.64                         7.7572  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 215.1133719944513\n",
      "MAE: 13.011396424167694\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o modelo que recebemos na AWS\n",
    "mse = mean_squared_error(y_test, X_test['delivery_time_model'])\n",
    "mae = mean_absolute_error(y_test, X_test['delivery_time_model'])\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para imprimir a data específica que estamos prevendo que o pedido será entregue, mas como não precisamos disso no kaggle estou deixando de lado por enquanto\n",
    "\n",
    "'PrevisaoEntrega': [X_test['order_purchase_timestamp'].iloc[i] + timedelta(days=pred) for i, pred in enumerate(y_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/DataFramelimpa_sem_latlong.csv\")\n",
    "\n",
    "# Pré-processamento dos Dados\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calcular o volume do produto\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "df = calculate_zip_similarity(df)\n",
    "\n",
    "# Preencher valores ausentes com a mediana das colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Variáveis preditoras e alvo, incluindo order_id e order_purchase_timestamp para uso futuro\n",
    "X = df[['order_id', 'order_purchase_timestamp', 'delivery_time_model', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "y = df['delivery_time']  # Usar delivery_time já existente\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo com regressor RandomForestRegressor\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Definir a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Treinar o modelo com GridSearch\n",
    "grid_search.fit(X_train.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']), y_train)\n",
    "\n",
    "# Melhor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = best_model.predict(X_test.drop(columns=['order_id', 'order_purchase_timestamp', 'delivery_time_model']))\n",
    "\n",
    "# Diagnóstico para verificar as previsões\n",
    "print(\"Valores de previsão extremos:\", y_pred[(y_pred < 0) | (y_pred > 365)]) \n",
    "\n",
    "# Limitar valores de previsão a um intervalo razoável, por exemplo, 0 a 365 dias (1 ano)\n",
    "y_pred = np.clip(y_pred, 0, 365)\n",
    "\n",
    "# Criar DataFrame com as previsões e os resultados reais\n",
    "df_resultado = pd.DataFrame({\n",
    "    'OrderID': X_test['order_id'].values,\n",
    "    'Delivery Time Real': y_test,\n",
    "    'Delivery Time Previsto (Antigo)': X_test['delivery_time_model'],\n",
    "    'Delivery Time Previsto (Novo)': y_pred,\n",
    "})\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'Melhores hiperparâmetros: {grid_search.best_params_}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(best_model, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Salvar resultado como CSV\n",
    "df_resultado.to_csv('Data/resultado_previsoes.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões salvo com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV de previsões para o kaggle salvo com sucesso!\n",
      "                           order_id  order_metric_cycle_time\n",
      "0  cee3292f46ede6ea1dfabfcb200fcf47                   9.5672\n",
      "1  50dca53ca33b739bef09e7933e8b380e                  10.2296\n",
      "2  8087ec71e393d4dc6fc48041fe63cd51                   5.8191\n",
      "3  e6b6557ce111de79b31cc857f20ba212                  10.3058\n",
      "4  0b09c5e4c2512f627190ac55a78c35a3                  14.2867\n"
     ]
    }
   ],
   "source": [
    "# código que retora o csv do jeito que ele precisa ser submetido no kaggle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "# Função para calcular a similaridade dos prefixos de CEP\n",
    "def calculate_zip_similarity(df):\n",
    "    def compare_zip_codes(row):\n",
    "        seller_zip = str(row['seller_zip_code_prefix'])\n",
    "        customer_zip = str(row['customer_zip_code_prefix'])\n",
    "        similarity = sum(1 for s, c in zip(seller_zip, customer_zip) if s == c)\n",
    "        return similarity\n",
    "\n",
    "    df['zip_code_similarity'] = df.apply(compare_zip_codes, axis=1)\n",
    "    return df\n",
    "\n",
    "# Função para pré-processamento dos dados\n",
    "def preprocess_data(df):\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "    df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "    df['product_volume'] = df['product_length_cm_x'] * df['product_height_cm_x'] * df['product_width_cm_x']\n",
    "    df['product_weight_g'] = df['product_weight_g_x']\n",
    "    df = extract_datetime_features(df)\n",
    "    df = calculate_zip_similarity(df)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model_pipeline = joblib.load('model_pipeline.pkl')\n",
    "\n",
    "# Carregar os novos dados\n",
    "df_new = pd.read_csv(\"Data/kaggle_enriquecido.csv\")\n",
    "df_new = preprocess_data(df_new)\n",
    "\n",
    "# Variáveis preditoras\n",
    "X_new = df_new[['order_id', 'order_purchase_timestamp', 'purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay', 'product_volume', 'product_weight_g', 'zip_code_similarity', 'freight_value', 'price']]\n",
    "\n",
    "# Fazer previsões nos novos dados\n",
    "y_pred_new = model_pipeline.predict(X_new.drop(columns=['order_id', 'order_purchase_timestamp']))\n",
    "\n",
    "# Criar DataFrame com as previsões\n",
    "df_resultado_new = pd.DataFrame({\n",
    "    'order_id': X_new['order_id'].values,\n",
    "    'order_metric_cycle_time': y_pred_new\n",
    "})\n",
    "\n",
    "# Salvar resultado como CSV sem cabeçalhos\n",
    "df_resultado_new.to_csv('Data/resultado_previsoes_kaggle2.csv', index=False)\n",
    "\n",
    "print(\"CSV de previsões para o kaggle salvo com sucesso!\")\n",
    "print(df_resultado_new.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28152.000000\n",
       "mean        10.936476\n",
       "std          3.431379\n",
       "min          2.430900\n",
       "25%          8.398900\n",
       "50%         11.341250\n",
       "75%         13.487275\n",
       "max         22.471000\n",
       "Name: Delivery Time Previsão, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado_new = pd.read_csv('Data/resultado_previsoes_kaggle.csv')\n",
    "\n",
    "# Descrever a coluna 'Delivery Time Previsão'\n",
    "delivery_time_stats = df_resultado_new['Delivery Time Previsão'].describe()\n",
    "\n",
    "delivery_time_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas na DataFrame final: 59352\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar a DataFrame principal e a DataFrame order_items\n",
    "df_principal = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "df_order_items = pd.read_csv(\"Data/order_items.csv\")\n",
    "\n",
    "# Corrigir zip codes\n",
    "df_principal['seller_zip_code_prefix'] = df_principal['seller_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n",
    "df_principal['customer_zip_code_prefix'] = df_principal['customer_zip_code_prefix'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n",
    "\n",
    "# Remover linhas duplicadas da DataFrame order_items com base na coluna order_id\n",
    "df_order_items_clean = df_order_items.drop_duplicates(subset=['order_id'])\n",
    "\n",
    "# Realizar o merge usando pandas\n",
    "df_final = pd.merge(df_principal, df_order_items_clean, on='order_id', how='left')\n",
    "\n",
    "# Verificar o tamanho da DataFrame final\n",
    "print(f\"Número de linhas na DataFrame final: {len(df_final)}\")\n",
    "\n",
    "df_final.to_csv(\"Data/DataFrame_final_com_juncao.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18843.643010133725\n"
     ]
    }
   ],
   "source": [
    "# código que rodou por 13 horas procurando hiper parametros e retornou um mse quase igual\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import joblib\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv(\"Data/Dataframelimpa_sem_latlong.csv\")\n",
    "\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])\n",
    "\n",
    "# Função para extrair características de data/hora\n",
    "def extract_datetime_features(df):\n",
    "    df['purchase_weekday'] = df['order_purchase_timestamp'].dt.weekday\n",
    "    df['purchase_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['purchase_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    df['approval_delay'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # em horas\n",
    "    return df\n",
    "\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "# Variáveis preditoras e alvo\n",
    "X = df[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "y = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 3600  # Prever em horas\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline de pré-processamento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['approval_delay']),\n",
    "        ('cat', OneHotEncoder(), ['purchase_weekday', 'purchase_month', 'purchase_hour'])\n",
    "    ])\n",
    "\n",
    "# Pipeline completo\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Função para treinar um modelo com um conjunto de parâmetros\n",
    "def train_model(params):\n",
    "    model_pipeline.set_params(**params)\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    return model_pipeline\n",
    "\n",
    "# Usar ThreadPoolExecutor para paralelizar GridSearchCV\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "# Salvar o modelo para uso futuro\n",
    "joblib.dump(best_model, 'modelo_previsao_data_entrega.pkl')\n",
    "\n",
    "# Exemplo de uso do modelo salvo em novos dados\n",
    "modelo_carregado = joblib.load('modelo_previsao_data_entrega.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'])\n",
    "\n",
    "df_novo = extract_datetime_features(df)\n",
    "\n",
    "X_novo = df_novo[['purchase_weekday', 'purchase_month', 'purchase_hour', 'approval_delay']]\n",
    "previsoes = modelo_carregado.predict(X_novo)\n",
    "\n",
    "df_novo['previsao_data_entrega_horas'] = previsoes\n",
    "df_novo['previsao_data_entrega'] = df_novo['order_purchase_timestamp'] + pd.to_timedelta(df_novo['previsao_data_entrega_horas'], unit='h')\n",
    "\n",
    "print(df_novo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
